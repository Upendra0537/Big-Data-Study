Hadoop - Framework

HDFS - Hadoop Distributed File System - Storage
MapReduce - processess - Map(distributes to all nodes) and reduces(results)
YARN - OS of HADOOP - Yet Another Resource Negotiator


MR-1 - Components
Client
Job Tracker - Master Deamon
Node Tracker - Slave Deamon
HDFS


MR-2 - YARN
Client
Resource Manager - 
Application Manager - Runs the applications of client by taking to Resource manager for resources and Node Manager for execution on the Nodes
Node Manager
HDFS

Files ??
core-site.xml - localhost Configurations
hdfs-site.xml - Configures Replication factor and HDFS properties
mapred-site.xml - Configures Map reduce framework
yarn-site.xml 


start-all.sh - start all deamon
stop-all.sh - stop all deamon
jps - to check for all deamon status


PIG - Scripting and Programming on Apache Hadoop
	Pig Latin Programming - Mapper and Reduce programs
HIVE - Query and Summarize Data in HDFS
Spark - Programming language for running processess on Hadoop very fast with API

Distributed Platforms
HortonWorks - Supports Windows
Cloudera - 

NoSQL (Not only SQL) Databases
- Key Value - HBase
- Document Oriented - Documents(JASON) and Collections
- Wide Column Family - HBase, Cassandra (combining multiple columns to a single Family Column)
- Graph Databases - Neo4j, TitanDB

HDFS - Hadoop Distributed File System - Storage
	NameNode - Metadata of Nodes - Single
	DataNode - Actual Data - Multiple
		Stored as blocks - 128MB
[Upendra]$ ls  -> local file system
[Upendra]$ hdfs ds -ls -> HDFS file System
copy from local to hdfs
[Upendra]$ hdfs dfs -put <filename> /user/xyz/hdfs/
[Upendra]$ hadoop fsck /user/xyz/hdfs - gives the blocks information

NameNode - SPOF(Single Point of Failure)
Solution is to have two NameNode in Active and Passive status. 
Passive namenodes syncs with changes to Active Namenode
QuorumJournalManager - JournalNode have logs of the active NameNode.

NameNode Contains
Editlog - logs all the functions
FsImage - namespace Image(mapping of blocks)

Secondary NameNode - backup of NameNode


sudo su -hdfs - to change the ownership


MapReduce 
- Programming Framework
- Parallel Processing 
- Map Function
        - Each block is processed by each Mapper
        - Input - HDFS block
        - Output - Local Disk(Intermediate Output)
        - Contains Business Logic
- Reduce Functions
        - Input - Local Disk from Mapper
        - Output - HDFS block
        - Aggregator and sorting functions 


Input(HDFS) -> Split(block) -> RR(Record Reader Split in (k,v)) -> Partitioner (Shuffling) -> Reduce -> Output(HDFS)

MapReduce Life Cycle - Hadoop 1X
a) Client Submits the Job to JobTracker
b) JobTracker would give JobId to client 
c) Client has all the business logic(Map/Reduce Functions & Dependencies) which will be copies to HDFS to a temp Location with JobID file name(at run time and gets deleted later)
d) Program Runs with the help of Job Tracker
e) Job Tracker will send the request to NameNode to get the details of the data.
f) NameNode sends the Metadata of the data nodes where the data exist.
g) Job Tracker checks the heart beat of the data nodes for availability.
h) Job Tracker will commands the healthy Datanodes task tracker to run it 
i) and Task Trackers will copy the job from the Temp HDFS location with Job Id file name. - Each datanode will have task tracker(to run the job) and data
j) Start the JVM of the Task Trackers of Data Node and execute the job on the data in the datanode
k) the output of the datanodes and stored in a single machine 
l) Reducer function runs on data of the datanode and write the output to the HDFS


Commands
hadoop fs -ls => to check for the list of file in HDFS at /user/Upendra/
hadoop fs -mkdir store1  => Create a directory in HDFS at /user/Upendra/
hadoop fs -put a.txt store1  => to move the local file to HDFS
hadoop jar wcount.jar WordCount /user/upendra/store1/a.txt output1 => to run the local JAR in hadoop on HDFS
hadoop fs -cat /user/upendra/output1/part-r-00000  => to look into the reducer part in HDFS
hadoop fs -get /user/upendra/output1/part-r-00000  => to get the reducer to local system from HDFS

4 V's of Big Data

Volume
Varity
Variability
Velocity


Data Nodes in Hadoop 2X Contains
Application master
Container
Node Manager
Resource Manager


The implementation of compression and decompression algorithm is known as codecs.

Safe Mode ??
different files 





