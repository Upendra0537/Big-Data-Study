HQL - Hive Query Language

turns to MapReduce to communicate with Hadoop/HDFS
to query unstructured and semi-structured data on Hadoop.

OLAP - Online Analytic Processing
Cannot be used for OLTP (Online Transaction Processing)

Creates Schema on Reading the data from HDFSHive

Hive Supports multiple application using Driver
	Hive JDBC Driver
	Hive ODBC Driver
	Hive Thrift Application

Hive Driver
Hive Server
MetaStore - Schema is stored
	- Embedded - Derby - one session at a time(not suitable for Production)
	- Local Metastore - MySQL
	- Remote Metastore


Execution Process
-> UI/CLI where the Hive query is written and given to driver
-> Drive communicates with Compiler using Metastore and create a plan
-> Plan is shared to the Execution Engine which also uses MetaStore and talks Hadoop for data retrieval

Default Database - metastore_db
case-sensitive

Collections
	- Map - Key,value
	- Struct - object reference
	- Array - index

hive -f <scriptname> - to run a script

Databases - NameSpace
	-> Tables
		-> Partition
			-> Buckets

DDL:
Database


USE <databasename>





Create:
Create Table 
External - When dropped data exist, Metadata is dropped. data from external sources. Needs to specify the location for data retrieval
Internal/Managed Table - data is stored in default path /user/warehouse/hive


CREATE DATABASE <databasename>
LOCATION 'my/user/'
COMMENT 'this is my database'
WITH DBPROPERTIES('createdby' = 'upendra', 'date' = '2020-01-01')

create table tablename(col1 int,col2 string) row format delimited fields terminated by ','

create view <viewname> as select * from <tablename>

CREATE FUNCTION [IF NOT EXISTS] [db_name.]function_name([arg_type[, arg_type...])
  RETURNS return_type
  LOCATION 'hdfs_path'
  SYMBOL='symbol_or_class'

CREATE INDEX index_col ON TABLE tablename(col)
AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';

Drop:

DROP DATABASE IF EXIST <databasename>

DROP TABLE IF EXISTS <tablename>;

DROP view <viewname>;

DROP INDEX <index_name> ON <table_name>;

Alter:

ALTER DATABASE test_db RENAME TO test_db_new;

alter table <tablename>  add columns(car int);

ALTER DATABASE <databasename> SET DBPROPERTIES('createdby' = 'upendra', 'date' = '2020-01-01') - only to update properties


SHOW DATABASES;

show table;

show partitions databasename.tablename;

show create view databasename.viewname;

DESCRIBE DATABASE <databasename>

DESCRIBE DATABASE EXTENDED <databasename>

DESCRIBE TABLE


DML:

When you want to load data from your local machine,
load data local inpath [FilePath] into table <tablename>
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
When you want to load data directly from HDFS,
load data  inpath  into table <tablename>


hive> create table records (name string,age int,cgpa decimal(3,2))clustered by (age) into 2 buckets stored as orc tblproperties('transactional'='true');

hive> insert into table records values('john Cena',45,8.02),('batista',48,8.96);

hive> select * from records;
OK
batista    48    8.96
john Cena    45    8.02
Time taken: 0.109 seconds, Fetched: 2 row(s)

hive> update records set name='justin' where age=45;
select * from records;
hive> select * from records;
OK
batista	48	8.96
justin	45	8.02


hive> merge into employee
    > using (select * from empmerge) sub
    > on sub.id=employee.id
    > when matched then update set name=sub.name,place=sub.place
    > when not matched then insert values(sub.id,sub.name,sub.state);


set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.enforce.bucketing=true;
set hive.exec.dynamic.partition.mode=nonstrict

select dept,count(*) from person group by dept;
 select * from person order by salary DESC;

select id, name from person distribute by id; same categories go to single file/reducer

select id, name from person cluster by id;

JOINS - dont handle non equal join

select sales.* , product.*
    > FROM sales JOIN product ON(sales.id=product.id);

select sales.*,product.* 
    > from sales 
    > left outer join 
    > product on (sales.id=product.id);

select sales.*,product.* 
    > from sales 
    > right outer join product on (sales.id=product.id);

select sales.*,product.* 
    > from sales 
    > full outer join product on (sales.id=product.id);

select * 
    > from product
    > left semi join sales on(sales.id=product.id);

create table <tablename> as select sales.*,product.* 
    > from sales 
    > right outer join product on (sales.id=product.id);

Map Side Join - less time, with constrains
Reduce Side Join - flexible, lot of time


Partitioning - to improve performance - does the activity only on that partition
- it is directory

Static Partitioning 
Dynamic partitioning.

=>set hive.exec.dynamic.partition.mode=nonstrict;
(as hive is by default set to strict mode)
=>load data local inpath '/home/arani/Desktop/cities.csv' overwrite into table city partition(city="chennai");
=>load data local inpath '/home/arani/Desktop/cities2.csv' overwrite into table city partition(city="mumbai");

=>select * from city where city="chennai";

Bucket - to improve the latency of partitioning
- is a file
- we can specify the number of the buckets

create table bucket_table(id int,firstname string,lastname string) clustered by (id) into 5 buckets row format delimited fields terminated by ',';

hive>insert overwrite table bucket_table select * from table1;

hadoopusr$ hdfs dfs -cat /user/hive/warehouse/bucketeddb.db/bucket_table/000000_0 in terminal.
set hive.enforce.bucketing=true;
set hive.exec.dynamic.partition.mode=nonstrict;

hive> create table partition_table(id int,lastname string) partitioned by(firstname string) clustered by (id) into 5 buckets row format delimited fields terminated by ',';

Data Sampling
select * from bucket_table tablesample(bucket 1 out of 5 on id);

UDF - User Defined Functions
-> creals
te a java class and create a jar file.
-> must import org.apache.hadoop.hive.ql.exec.UDF

hive> add jar /home/arani/Desktop/HIVE_DOCUMENTATION/UDFExample.jar;

hive> create temporary function removeCharacter as 'UDFExample';

hive> select removeCharacter(name,'$') from udf_example;

UDAF - User Defined Aggregate Funtion

1.init().

2.iterate().

3.terminatePartial().

4.merge().

5.terminate().

UDTF - User Defined Tabular Function

1.initialize()

2.process()

3.close()



CREATE TABLE distributor(District_ID varchar, Distributor_Name varchar, Buy_Rate varchar, Sell_Rate varchar, VolumeIN int, VolumeOUT int, Year int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;

LOAD DATA INPATH '/user/hadoop/tmp/oil.txt' OVERWRITE TABLE INTO distributor

SELECT Distributor_Name, sum(cast(Sell_Rate as int) * VolumeOUT) AS Selling_Amount FROM distributor GROUP BY Distributor_Name ORDER BY Selling_Amount DESC LIMIT 10;
 	
SELECT Distributor_Name, MIN(Sell_Rate), SUM(VolumeOUT * cast(Sell_Rate as int) FROM distributor GROUP BY Distributor_name

select Distributor_name, sum(volumeOUT) from distributor group by Distributor_name, Year 


create table games(AthleteName String, Age int, Country String, Year int, Closing_Date String, Sport String, Gold_Medals int, Silver_Medals int, Bronze_Medals int, Total_Medals int) row format delimited fields terminated by ',';

load data inpath ' ' into games

select Country, sum(Total_Medals) from games where Sport = 'Swimming' group by Country

select Year, sum(Total_Medals) from games where Country='India' group by Year

select Country, sum(Total_Medals) from games group by Country

select Country, sum(Gold_Medals) from games group by Country

select Country, Year from games where Sport='Shooting' group by Year

sudo service cloudera-scm-agent restart


sudo service cloudera-scm-agent restart
sudo hdfs dfsadmin -safemode leave

hdfs dfsadmin -safemode get

sudo -u hdfs hdfs fsck / -listcourrptblocks

https://13.233.158.140:7180/cmf/home

@Sameeth Shetty . I spent around 6 hrs trying to figure/solve the fresco playground for cloudera. Still having lot many questions. I have read through the discussion notes but did not help. Can you share clear direction on a) Connection Details b) Common Errors and basic troubleshooting c) Sample data for the Hand's ON.  

Rohini Dasgupta


Name	age	country	Year	Closing_Date	Sport	Gold_Medals	Silver_Medals	Bronze_Medals	Total_Medals
Michael Dee	23	United States	2009	2009-02-03	Swimming	23	3	2	28
Larisa Latynina	26	Soviet Union	1964	1964-03-02	Gymnastics	9	5	4	16
Takashi Ono	30	Japan	 1975	1975-06-09	Shooting	5	4	4	13
Matt Biondi	32	United States	1992	1992-01-06	Swimming	8	2	1	11
Natalie Coughlin	52	India	1960	1960-02-05	Swimming	3	2	2	7
Ram Dee	23	United States	2009	2009-02-03	Swimming	23	3	2	28
Larisa lotta	26	Soviet Union	1964	1964-03-02	Shooting	9	5	4	16
Taboo Ono	30	Japan	 1975	1975-06-09	Gymnastics	5	4	4	13
Matt Bonda	32	United States	1980	1980-01-06	Shooting	8	2	1	11
Nata Mati	52	India	1962	1962-02-05	Swimming	3	2	2	7	
Nam ko	52	India	1960	1960-02-05	Swimming	3	2	2	7
la doo	23	United States	2009	2009-02-03	Swimming	23	3	2	28
kim koo	26	India	1964	1964-03-02	Shooting	9	5	4	16
Taboo minu 30	Japan	 1975	1975-06-09	Gymnastics	5	4	4	13
ravo Bonda	32	United States	1980	1980-01-06	Shooting	8	2	1	11
kati Mati	52	India	1969	1969-02-05	Swimming	3	2	2	7	



Id	Name	Buyrate	sellrate	volumeIn	VolumeOut	Year
001M1	shell	957.70	5779.92	933	843	1624
001M2	bpl	675.24	3400.78	600	780	1625
001M3	Hindustan	650.00	5000.00	852	952	1750
001M4	Indian	789.52	7232.86	216	895	1760
002M1	shell	934.70	5649.92	567	898	1755
002M2	bpl	689.24	3780.78	578	870	1765
002M3	Hindustan	750.00	5780.00	852	990	1780
002M4	Indian	389.52	7982.86	216	895	17970
011M1	shell	263.70	5979.92	933	843	1750
011M2	bpl	235.24	3400.78	600	780	1752
011M3	Hindustan	650.00	7800.00	852	952	1750
011M4	Indian	239.52	3432.86	216	895	1767
012M1	shell	744.70	9749.92	567	898	1753
012M2	bpl	979.24	9780.78	578	870	1769
012M3	Hindustan	350.00	8780.00	852	990	1783
012M4	Indian	453.52	7952.86	216	895	1791

